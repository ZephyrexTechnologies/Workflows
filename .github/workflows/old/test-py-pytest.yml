name: Reusable Compare Pytest Results

on:
  workflow_call:
    inputs:
      target_branch_to_compare:
        description: "The target branch to compare against (e.g., main, refs/heads/main)."
        required: true
        type: string
      python-version:
        description: "Python version to use for testing."
        required: false
        type: string
        default: "3.10"
      ping_latest_committer:
        description: "If true, the latest committer on the PR will be added to the ping list."
        required: false
        type: boolean
        default: false
      runs_on:
        required: false
        type: string
        default: "ubuntu-latest"
    secrets:
      DISCORD_WEBHOOK_URL:
        description: "Discord Webhook URL for failure notifications. If not provided, notifications are skipped."
        required: false
      DISCORD_USER_MAP:
        description: 'JSON string mapping GitHub usernames to Discord User IDs (e.g., {"user1":"id1"}). If not provided, users won''t be pinged.'
        required: false
    outputs:
      pr_total:
        description: "Total tests in PR/source branch"
        value: ${{ jobs.test-source-branch.outputs.total }}
      pr_passed:
        description: "Passed tests in PR/source branch"
        value: ${{ jobs.test-source-branch.outputs.passed }}
      pr_percentage:
        description: "Pass percentage in PR/source branch"
        value: ${{ jobs.test-source-branch.outputs.percentage }}
      pr_failing_tests:
        description: "List of failing tests in PR/source branch (JSON string)"
        value: ${{ jobs.test-source-branch.outputs.failing_tests }}
      pr_collection_errors:
        description: "PR branch has collection errors"
        value: ${{ jobs.test-source-branch.outputs.collection_errors }}
      pr_no_tests_found:
        description: "PR branch has no tests found"
        value: ${{ jobs.test-source-branch.outputs.no_tests_found }}
      target_total:
        description: "Total tests in target branch"
        value: ${{ jobs.test-target-branch.outputs.total }}
      target_passed:
        description: "Passed tests in target branch"
        value: ${{ jobs.test-target-branch.outputs.passed }}
      target_percentage:
        description: "Pass percentage in target branch"
        value: ${{ jobs.test-target-branch.outputs.percentage }}
      target_passing_tests:
        description: "List of passing tests in target branch (JSON string)"
        value: ${{ jobs.test-target-branch.outputs.passing_tests }}
      has_regressions:
        description: "Boolean indicating if regressions were found"
        value: ${{ jobs.compare-results.outputs.has_regressions }}
      regression_count:
        description: "Number of test regressions found"
        value: ${{ jobs.compare-results.outputs.regression_count }}

jobs:
  lint:
    uses: ./.github/workflows/test-py-lint.yml
    permissions:
      contents: write # Lint job might push changes

  test-source-branch:
    needs: lint
    runs-on: ${{ inputs.runs_on }}
    outputs:
      total: ${{ steps.extract-results.outputs.total }}
      passed: ${{ steps.extract-results.outputs.passed }}
      percentage: ${{ steps.extract-results.outputs.percentage }}
      collection_errors: ${{ steps.check-collection.outputs.has_collection_errors }}
      no_tests_found: ${{ steps.check-collection.outputs.no_tests_found }}
      has_errors: ${{ steps.check-collection.outputs.has_errors }}
      error_type: ${{ steps.check-collection.outputs.error_type }}
      error_details: ${{ steps.check-collection.outputs.error_details }}
      failing_tests: ${{ steps.extract-results.outputs.failing_tests }}
      skipped_tests: ${{ steps.extract-results.outputs.skipped_tests }}
      xfailed_tests: ${{ steps.extract-results.outputs.xfailed_tests }}
      all_tests: ${{ steps.extract-results.outputs.all_tests }}
      skipped_tests_with_reasons: ${{ steps.extract-results.outputs.skipped_tests_with_reasons }}
      xfailed_tests_with_reasons: ${{ steps.extract-results.outputs.xfailed_tests_with_reasons }}

    steps:
      - name: Checkout PR Branch
        uses: actions/checkout@v4.2.2
        with:
          submodules: "recursive"

      - name: Set up Python
        uses: actions/setup-python@v5.3.0
        with:
          python-version: "${{ inputs.python-version }}"

      - name: Install dependencies
        run: |
          set -x
          python -m pip install --upgrade pip
          pip install pytest pytest-json-report pytest-asyncio
          if [ -f requirements.txt ]; then 
            pip install -r requirements.txt
          fi

      - name: Check for test collection errors
        id: check-collection
        run: |
          set -x
          # Create verbose debug file for artifact
          exec 3>&1 4>&2
          exec 1> >(tee -a debug_collection.log) 2>&1

          echo "Running pytest collection check..."
          python -m pytest --collect-only -v > collection_output.txt 2>&1 || true

          # Restore stdout/stderr for GitHub Actions
          exec 1>&3 2>&4

          # Set default values
          HAS_COLLECTION_ERRORS="false"
          NO_TESTS_FOUND="false"
          ERROR_TYPE="none"
          ERROR_DETAILS="none"

          if grep -q "ImportError\|ModuleNotFoundError\|SyntaxError\|ERROR collecting\|Interrupted:" collection_output.txt; then
            echo "::error::Test discovery errors detected in PR branch - Python modules could not be imported correctly"
            
            # Attempt to identify specific error type
            if grep -q "ImportError" collection_output.txt; then
              ERROR_TYPE="ImportError"
            elif grep -q "ModuleNotFoundError" collection_output.txt; then
              ERROR_TYPE="ModuleNotFoundError"
            elif grep -q "SyntaxError" collection_output.txt; then
              ERROR_TYPE="SyntaxError"
            elif grep -q "ERROR collecting" collection_output.txt; then
              ERROR_TYPE="CollectionError"
            elif grep -q "Interrupted:" collection_output.txt; then
              ERROR_TYPE="Interrupted"
            else 
              ERROR_TYPE="UnknownError"
            fi
            
            echo "PR branch discovery error type: $ERROR_TYPE"
            
            ERROR_FILE=$(grep -o "ERROR collecting.*\.py" collection_output.txt | grep -o "[a-zA-Z0-9_/]*\.py" || echo "Unknown file")
            
            if [[ "$ERROR_FILE" != "Unknown file" ]]; then
              echo "Error in file $ERROR_FILE"
              grep -A 15 "$ERROR_FILE" collection_output.txt > error_details.txt
              ERROR_DETAILS=$(cat error_details.txt | tr '\n' ' ' | sed 's/"/\\"/g')
            else
              # If we couldn't find a specific file, get general error info
              grep -A 15 "ImportError\|ModuleNotFoundError\|SyntaxError\|ERROR collecting\|Interrupted:" collection_output.txt | head -20 > error_details.txt
              ERROR_DETAILS=$(cat error_details.txt | tr '\n' ' ' | sed 's/"/\\"/g')
            fi
            
            echo "::error::PR branch discovery error details: ${ERROR_DETAILS:0:200}..."
            HAS_COLLECTION_ERRORS="true"
          else
            echo "No discovery errors detected in PR branch"
            
            TEST_COUNT=$(grep -o "collected [0-9]* item" collection_output.txt | grep -o "[0-9]*" || echo "0")
            
            if [[ "$TEST_COUNT" == "0" ]]; then
              echo "::warning::No tests were found in the PR branch"
              NO_TESTS_FOUND="true"
              ERROR_TYPE="NoTestsFound"
              ERROR_DETAILS="No test files were discovered that match pytest's test discovery pattern"
            else  
              echo "Found $TEST_COUNT tests in PR branch"
            fi
          fi

          # Set all the outputs
          echo "has_collection_errors=$HAS_COLLECTION_ERRORS" >> $GITHUB_OUTPUT
          echo "no_tests_found=$NO_TESTS_FOUND" >> $GITHUB_OUTPUT
          echo "error_type=$ERROR_TYPE" >> $GITHUB_OUTPUT
          echo "error_details=$ERROR_DETAILS" >> $GITHUB_OUTPUT

          # For backward compatibility
          if [[ "$HAS_COLLECTION_ERRORS" == "true" || "$NO_TESTS_FOUND" == "true" ]]; then
            echo "has_errors=true" >> $GITHUB_OUTPUT
          else
            echo "has_errors=false" >> $GITHUB_OUTPUT
          fi

          # Save full collection output to debug file for artifact
          echo "=== FULL COLLECTION OUTPUT ===" >> debug_collection.log
          cat collection_output.txt >> debug_collection.log

      - name: Run tests on PR Branch
        if: steps.check-collection.outputs.has_collection_errors != 'true'
        run: |
          set -x
          # Create verbose debug file for artifact
          exec 3>&1 4>&2
          exec 1> >(tee -a debug_test_run.log) 2>&1

          echo "Running tests on PR branch..."
          python -m pytest -vv --json-report --json-report-file=pr_results.json || true

          # Restore stdout/stderr for GitHub Actions
          exec 1>&3 2>&4

          if [ -f pr_results.json ]; then
            echo "Test results file successfully created for PR branch"
            # Save first 200 chars to debug file only
            echo "=== JSON RESULTS PREVIEW ===" >> debug_test_run.log
            head -c 200 pr_results.json >> debug_test_run.log
          else
            echo "::error::Failed to create test results file for PR branch"
          fi

      - name: Extract test results
        id: extract-results
        run: |
          set -x
          echo "PR_BRANCH=$(git rev-parse --abbrev-ref HEAD)" >> $GITHUB_ENV
          echo "Processing test results for PR branch: $PR_BRANCH"

          # Create debug file for detailed output
          exec 3>&1 4>&2
          exec 1> >(tee -a debug_extract_results.log) 2>&1

          python -c "
          import json
          import sys
          import os

          print('Starting test results extraction script for PR branch')

          # Default values in case file doesn't exist or is invalid
          pr_total = 0
          pr_passed = 0
          pr_percentage = 0
          failing_tests = []
          skipped_tests = []
          xfailed_tests = []
          all_tests = []
          skipped_tests_with_reasons = {}
          xfailed_tests_with_reasons = {}

          try:
              print('Attempting to open pr_results.json')
              with open('pr_results.json') as f:
                  pr_results = json.load(f)
                  print(f'JSON loaded successfully, keys: {list(pr_results.keys())}')
              
              # Check for collection errors by looking at exitcode or error patterns
              if pr_results.get('exitcode', 0) > 1:
                  print('Detected non-zero exitcode, likely a collection error')
                  if 'collectors' in pr_results and pr_results['collectors']:
                      print(f'Collection errors found: {pr_results[\"collectors\"]}')
                  pr_total = 0  # Explicitly set to 0 - no tests run when collection fails
                  pr_passed = 0
              elif 'summary' in pr_results and isinstance(pr_results['summary'], dict):
                  # Normal case - extract data from summary
                  summary = pr_results['summary']
                  pr_total = summary.get('total', 0)
                  pr_passed = summary.get('passed', 0)
                  print(f'Results extracted from summary - Total: {pr_total}, Passed: {pr_passed}')
                  
                  # Extract all tests by outcome and collect all test nodeids with reasons
                  if 'tests' in pr_results:
                      print('Extracting failing, skipped, xfailed, and all tests with reasons')
                      for test in pr_results['tests']:
                          outcome = test.get('outcome')
                          nodeid = test.get('nodeid', '')
                          if nodeid:
                              all_tests.append(nodeid)  # Track all tests regardless of outcome
                              if outcome in ['failed', 'error']:
                                  failing_tests.append(nodeid)
                              elif outcome == 'skipped':
                                  skipped_tests.append(nodeid)
                                  # Extract skip reason
                                  skip_reason = 'No reason provided'
                                  if 'longrepr' in test and test['longrepr']:
                                      # longrepr can be a string or list, handle both
                                      longrepr = test['longrepr']
                                      if isinstance(longrepr, list) and longrepr:
                                          skip_reason = str(longrepr[0]) if longrepr[0] else 'No reason provided'
                                      elif isinstance(longrepr, str):
                                          skip_reason = longrepr
                                  elif 'call' in test and test['call'] and 'longrepr' in test['call']:
                                      skip_reason = str(test['call']['longrepr'])
                                  skipped_tests_with_reasons[nodeid] = skip_reason.strip()
                              elif outcome == 'xfailed':
                                  xfailed_tests.append(nodeid)
                                  # Extract xfail reason
                                  xfail_reason = 'No reason provided'
                                  if 'longrepr' in test and test['longrepr']:
                                      longrepr = test['longrepr']
                                      if isinstance(longrepr, list) and longrepr:
                                          xfail_reason = str(longrepr[0]) if longrepr[0] else 'No reason provided'
                                      elif isinstance(longrepr, str):
                                          xfail_reason = longrepr
                                  elif 'call' in test and test['call'] and 'longrepr' in test['call']:
                                      xfail_reason = str(test['call']['longrepr'])
                                  xfailed_tests_with_reasons[nodeid] = xfail_reason.strip()
                      
                      print(f'Found {len(failing_tests)} failing tests')
                      print(f'Found {len(skipped_tests)} skipped tests')
                      print(f'Found {len(xfailed_tests)} xfailed tests')
                      print(f'Found {len(all_tests)} total discovered tests')
              else:
                  print('No valid summary structure found')
              
              # Calculate percentage safely
              pr_percentage = (pr_passed / pr_total * 100) if pr_total > 0 else 0
              print(f'Pass percentage calculated: {pr_percentage:.2f}%')
              
          except FileNotFoundError as e:
              print(f'File not found error: {e}')
          except KeyError as e:
              print(f'Missing key in results file: {e}')
              if 'pr_results' in locals():
                  print(f'Available keys: {list(pr_results.keys())}')
                  if 'summary' in pr_results:
                      print(f'Summary structure: {pr_results[\"summary\"]}')
          except Exception as e:
              print(f'Error processing results: {e}')
              import traceback
              print(f'Full exception: {traceback.format_exc()}')

          print(f'Total tests: {pr_total}')
          print(f'Passed tests: {pr_passed}')
          print(f'Pass percentage: {pr_percentage:.2f}%')
          print(f'Failing tests: {len(failing_tests)}')
          print(f'Skipped tests: {len(skipped_tests)}')
          print(f'Xfailed tests: {len(xfailed_tests)}')
          print(f'All discovered tests: {len(all_tests)}')

          # Set outputs for GitHub Actions
          print('Writing results to GITHUB_OUTPUT')
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f'total={pr_total}\\n')
              f.write(f'passed={pr_passed}\\n')
              f.write(f'percentage={pr_percentage:.2f}\\n')
              # Write test lists as compact JSON strings to avoid issues with large outputs
              if failing_tests:
                  f.write(f'failing_tests={json.dumps(failing_tests)}\\n')
              else:
                  f.write('failing_tests=[]\\n')
              if skipped_tests:
                  f.write(f'skipped_tests={json.dumps(skipped_tests)}\\n')
              else:
                  f.write('skipped_tests=[]\\n')
              if xfailed_tests:
                  f.write(f'xfailed_tests={json.dumps(xfailed_tests)}\\n')
              else:
                  f.write('xfailed_tests=[]\\n')
              if all_tests:
                  f.write(f'all_tests={json.dumps(all_tests)}\\n')
              else:
                  f.write('all_tests=[]\\n')
              # Write test reason mappings
              if skipped_tests_with_reasons:
                  f.write(f'skipped_tests_with_reasons={json.dumps(skipped_tests_with_reasons)}\\n')
              else:
                  f.write('skipped_tests_with_reasons={}\\n')
              if xfailed_tests_with_reasons:
                  f.write(f'xfailed_tests_with_reasons={json.dumps(xfailed_tests_with_reasons)}\\n')
              else:
                  f.write('xfailed_tests_with_reasons={}\\n')

          print('Results extraction completed')
          "

          # Restore stdout/stderr for GitHub Actions
          exec 1>&3 2>&4

          echo "PR branch test results processed: ${{ steps.extract-results.outputs.passed }}/${{ steps.extract-results.outputs.total }} tests passed (${{ steps.extract-results.outputs.percentage }}%)"

      - name: Upload PR branch debug logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: pr_branch_debug_logs_${{ github.event.pull_request.number || github.run_id }}
          path: |
            debug_*.log
            collection_output.txt
            error_details.txt
            pr_results.json
          retention-days: 3
          if-no-files-found: ignore

      - name: Create PR branch test data artifact
        if: always()
        run: |
          set -x
          echo "Creating PR branch test data for regression analysis..."

          # Create directory for artifact
          mkdir -p pr_test_data

          # Extract failing tests from outputs and save to JSON file
          python3 - << 'EOF'
          import json
          import os

          # Get failing tests from the step outputs (if available)
          failing_tests_str = '''${{ steps.extract-results.outputs.failing_tests || '[]' }}'''

          try:
              if failing_tests_str and failing_tests_str != '[]':
                  failing_tests = json.loads(failing_tests_str)
              else:
                  failing_tests = []
              
              print(f"Saving {len(failing_tests)} failing tests to artifact")
              
              # Save to JSON file for artifact
              with open('pr_test_data/failing_items.json', 'w') as f:
                  json.dump(failing_tests, f, indent=2)
              
              print("PR branch test data saved successfully")
              
          except Exception as e:
              print(f"Error saving PR branch test data: {e}")
              # Create empty file as fallback
              with open('pr_test_data/failing_items.json', 'w') as f:
                  json.dump([], f)
          EOF

      - name: Upload PR branch test data artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: pr_branch_test_data_${{ github.event.pull_request.number || github.run_id }}
          path: pr_test_data/
          retention-days: 1

  test-target-branch:
    needs: lint
    runs-on: ${{ inputs.runs_on }}
    outputs:
      total: ${{ steps.check-collection.outputs.has_collection_errors == 'true' && steps.set-error-outputs.outputs.total || steps.extract-results.outputs.total }}
      passed: ${{ steps.check-collection.outputs.has_collection_errors == 'true' && steps.set-error-outputs.outputs.passed || steps.extract-results.outputs.passed }}
      percentage: ${{ steps.check-collection.outputs.has_collection_errors == 'true' && steps.set-error-outputs.outputs.percentage || steps.extract-results.outputs.percentage }}
      collection_errors: ${{ steps.check-collection.outputs.has_collection_errors }}
      no_tests_found: ${{ steps.check-collection.outputs.no_tests_found }}
      has_errors: ${{ steps.check-collection.outputs.has_errors }}
      error_type: ${{ steps.check-collection.outputs.error_type }}
      error_details: ${{ steps.check-collection.outputs.error_details }}
      passing_tests: ${{ steps.check-collection.outputs.has_collection_errors == 'true' && steps.set-error-outputs.outputs.passing_tests || steps.extract-results.outputs.passing_tests }}
      all_tests: ${{ steps.check-collection.outputs.has_collection_errors == 'true' && steps.set-error-outputs.outputs.all_tests || steps.extract-results.outputs.all_tests }}

    steps:
      - name: Checkout target branch
        uses: actions/checkout@v4.2.2
        with:
          submodules: "recursive"
          ref: ${{ inputs.target_branch_to_compare }}

      - name: Set up Python
        uses: actions/setup-python@v5.3.0
        with:
          python-version: "${{ inputs.python-version }}"

      - name: Install dependencies
        run: |
          set -x
          python -m pip install --upgrade pip
          pip install pytest pytest-json-report pytest-asyncio
          if [ -f requirements.txt ]; then 
            pip install -r requirements.txt
          fi

      - name: Check for test collection errors
        id: check-collection
        run: |
          set -x
          # Create verbose debug file for artifact
          exec 3>&1 4>&2
          exec 1> >(tee -a debug_target_collection.log) 2>&1

          echo "Running pytest collection check..."
          python -m pytest --collect-only -v > collection_output.txt 2>&1 || true

          # Restore stdout/stderr for GitHub Actions
          exec 1>&3 2>&4

          # Set default values
          HAS_COLLECTION_ERRORS="false"
          NO_TESTS_FOUND="false"
          ERROR_TYPE="none"
          ERROR_DETAILS="none"

          if grep -q "ImportError\|ModuleNotFoundError\|SyntaxError\|ERROR collecting\|Interrupted:" collection_output.txt; then
            echo "::warning::Test discovery errors detected in target branch - Python modules could not be imported correctly"
            
            # Attempt to identify specific error type
            if grep -q "ImportError" collection_output.txt; then
              ERROR_TYPE="ImportError"
            elif grep -q "ModuleNotFoundError" collection_output.txt; then
              ERROR_TYPE="ModuleNotFoundError"
            elif grep -q "SyntaxError" collection_output.txt; then
              ERROR_TYPE="SyntaxError"
            elif grep -q "ERROR collecting" collection_output.txt; then
              ERROR_TYPE="CollectionError"
            elif grep -q "Interrupted:" collection_output.txt; then
              ERROR_TYPE="Interrupted"
            else 
              ERROR_TYPE="UnknownError"
            fi
            
            echo "Target branch discovery error type: $ERROR_TYPE"
            
            ERROR_FILE=$(grep -o "ERROR collecting.*\.py" collection_output.txt | grep -o "[a-zA-Z0-9_/]*\.py" || echo "Unknown file")
            
            if [[ "$ERROR_FILE" != "Unknown file" ]]; then
              echo "Error in file $ERROR_FILE"
              grep -A 15 "$ERROR_FILE" collection_output.txt > error_details.txt
              ERROR_DETAILS=$(cat error_details.txt | tr '\n' ' ' | sed 's/"/\\"/g')
            else
              # If we couldn't find a specific file, get general error info
              grep -A 15 "ImportError\|ModuleNotFoundError\|SyntaxError\|ERROR collecting\|Interrupted:" collection_output.txt | head -20 > error_details.txt
              ERROR_DETAILS=$(cat error_details.txt | tr '\n' ' ' | sed 's/"/\\"/g')
            fi
            
            echo "::warning::Target branch discovery error details: ${ERROR_DETAILS:0:200}..."
            HAS_COLLECTION_ERRORS="true"
          else
            echo "No discovery errors detected in target branch"
            
            TEST_COUNT=$(grep -o "collected [0-9]* item" collection_output.txt | grep -o "[0-9]*" || echo "0")
            
            if [[ "$TEST_COUNT" == "0" ]]; then
              echo "::warning::No tests were found in the target branch"
              NO_TESTS_FOUND="true"
              ERROR_TYPE="NoTestsFound"
              ERROR_DETAILS="No test files were discovered in target branch that match pytest's test discovery pattern"
            else  
              echo "Found $TEST_COUNT tests in target branch"
            fi
          fi

          # Set all the outputs
          echo "has_collection_errors=$HAS_COLLECTION_ERRORS" >> $GITHUB_OUTPUT
          echo "no_tests_found=$NO_TESTS_FOUND" >> $GITHUB_OUTPUT
          echo "error_type=$ERROR_TYPE" >> $GITHUB_OUTPUT
          echo "error_details=$ERROR_DETAILS" >> $GITHUB_OUTPUT

          # For backward compatibility
          if [[ "$HAS_COLLECTION_ERRORS" == "true" || "$NO_TESTS_FOUND" == "true" ]]; then
            echo "has_errors=true" >> $GITHUB_OUTPUT
          else
            echo "has_errors=false" >> $GITHUB_OUTPUT
          fi

          # Save full collection output to debug file for artifact
          echo "=== FULL COLLECTION OUTPUT ===" >> debug_target_collection.log
          cat collection_output.txt >> debug_target_collection.log

      - name: Run tests on target branch
        if: steps.check-collection.outputs.has_collection_errors != 'true'
        run: |
          set -x
          # Create verbose debug file for artifact
          exec 3>&1 4>&2
          exec 1> >(tee -a debug_target_test_run.log) 2>&1

          echo "Running tests on target branch..."
          python -m pytest -vv --json-report --json-report-file=target_results.json || true

          # Restore stdout/stderr for GitHub Actions
          exec 1>&3 2>&4

          if [ -f target_results.json ]; then
            echo "Test results file successfully created for target branch"
            # Save first 200 chars to debug file only
            echo "=== JSON RESULTS PREVIEW ===" >> debug_target_test_run.log
            head -c 200 target_results.json >> debug_target_test_run.log
          else
            echo "::warning::Failed to create test results file for target branch"
          fi

      - name: Extract test results
        id: extract-results
        # Only run if there were no collection errors
        if: steps.check-collection.outputs.has_collection_errors != 'true'
        run: |
          set -x
          echo "Processing test results for target branch: ${{ inputs.target_branch_to_compare }}"

          # Create debug file for detailed output
          exec 3>&1 4>&2
          exec 1> >(tee -a debug_target_extract_results.log) 2>&1

          python -c "
          import json
          import sys
          import os

          print('Starting test results extraction script for target branch')

          # Default values in case file doesn't exist or is invalid
          target_total = 0
          target_passed = 0
          target_percentage = 0
          passing_tests = []
          all_tests = []

          try:
              print('Attempting to open target_results.json')
              with open('target_results.json') as f:
                  target_results = json.load(f)
                  print(f'JSON loaded successfully, keys: {list(target_results.keys())}')
              
              # Check for collection errors by looking at exitcode or error patterns
              if target_results.get('exitcode', 0) > 1:
                  print('Detected non-zero exitcode, likely a collection error')
                  if 'collectors' in target_results and target_results['collectors']:
                      print(f'Collection errors found: {target_results[\"collectors\"]}')
                  target_total = 0  # Explicitly set to 0 - no tests run when collection fails
                  target_passed = 0
              elif 'summary' in target_results and isinstance(target_results['summary'], dict):
                  # Normal case - extract data from summary
                  summary = target_results['summary']
                  target_total = summary.get('total', 0)
                  target_passed = summary.get('passed', 0)
                  print(f'Results extracted from summary - Total: {target_total}, Passed: {target_passed}')
                  
                  # Extract passing tests and all tests
                  if 'tests' in target_results:
                      print('Extracting passing tests and all discovered tests')
                      for test in target_results['tests']:
                          outcome = test.get('outcome')
                          nodeid = test.get('nodeid', '')
                          if nodeid:
                              all_tests.append(nodeid)  # Track all tests regardless of outcome
                              if outcome == 'passed':
                                  passing_tests.append(nodeid)
                      
                      print(f'Found {len(passing_tests)} passing tests')
                      print(f'Found {len(all_tests)} total discovered tests')
              else:
                  print('No valid summary structure found')
              
              # Calculate percentage safely
              target_percentage = (target_passed / target_total * 100) if target_total > 0 else 0
              print(f'Pass percentage calculated: {target_percentage:.2f}%')
              
          except FileNotFoundError as e:
              print(f'File not found error: {e}')
          except KeyError as e:
              print(f'Missing key in results file: {e}')
              if 'target_results' in locals():
                  print(f'Available keys: {list(target_results.keys())}')
                  if 'summary' in target_results:
                      print(f'Summary structure: {target_results[\"summary\"]}')
          except Exception as e:
              print(f'Error processing results: {e}')
              import traceback
              print(f'Full exception: {traceback.format_exc()}')

          print(f'Total tests: {target_total}')
          print(f'Passed tests: {target_passed}')
          print(f'Pass percentage: {target_percentage:.2f}%')
          print(f'Passing tests: {len(passing_tests)}')
          print(f'All discovered tests: {len(all_tests)}')

          # Set outputs for GitHub Actions
          print('Writing results to GITHUB_OUTPUT')
          with open(os.environ['GITHUB_OUTPUT'], 'a') as f:
              f.write(f'total={target_total}\\n')
              f.write(f'passed={target_passed}\\n')
              f.write(f'percentage={target_percentage:.2f}\\n')
              # Write test lists as compact JSON strings to avoid issues with large outputs
              if passing_tests:
                  f.write(f'passing_tests={json.dumps(passing_tests)}\\n')
              else:
                  f.write('passing_tests=[]\\n')
              if all_tests:
                  f.write(f'all_tests={json.dumps(all_tests)}\\n')
              else:
                  f.write('all_tests=[]\\n')

          print('Results extraction completed')
          "

          # Restore stdout/stderr for GitHub Actions
          exec 1>&3 2>&4

          echo "Target branch test results processed: ${{ steps.extract-results.outputs.passed }}/${{ steps.extract-results.outputs.total }} tests passed (${{ steps.extract-results.outputs.percentage }}%)"

      - name: Upload target branch debug logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: target_branch_debug_logs_${{ github.event.pull_request.number || github.run_id }}
          path: |
            debug_target_*.log
            collection_output.txt
            error_details.txt
            target_results.json
          retention-days: 3
          if-no-files-found: ignore

      - name: Create target branch test data artifact
        if: always()
        run: |
          set -x
          echo "Creating target branch test data for regression analysis..."

          # Create directory for artifact
          mkdir -p target_test_data

          # Extract passing tests from outputs and save to JSON file
          python3 - << 'EOF'
          import json
          import os

          # Get passing tests from the step outputs (if available)
          passing_tests_str = '''${{ steps.extract-results.outputs.passing_tests || steps.set-error-outputs.outputs.passing_tests || '[]' }}'''

          try:
              if passing_tests_str and passing_tests_str != '[]':
                  passing_tests = json.loads(passing_tests_str)
              else:
                  passing_tests = []
              
              print(f"Saving {len(passing_tests)} passing tests to artifact")
              
              # Save to JSON file for artifact
              with open('target_test_data/passing_items.json', 'w') as f:
                  json.dump(passing_tests, f, indent=2)
              
              print("Target branch test data saved successfully")
              
          except Exception as e:
              print(f"Error saving target branch test data: {e}")
              # Create empty file as fallback
              with open('target_test_data/passing_items.json', 'w') as f:
                  json.dump([], f)
          EOF

      - name: Upload target branch test data artifact
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: target_branch_test_data_${{ github.event.pull_request.number || github.run_id }}
          path: target_test_data/
          retention-days: 1

      # Add a step to set default outputs when collection errors are detected
      - name: Set collection error outputs
        id: set-error-outputs
        if: steps.check-collection.outputs.has_collection_errors == 'true'
        run: |
          echo "::warning::Setting default outputs for target branch due to collection errors"
          echo "total=0" >> $GITHUB_OUTPUT
          echo "passed=0" >> $GITHUB_OUTPUT
          echo "percentage=0.00" >> $GITHUB_OUTPUT
          echo "passing_tests=[]" >> $GITHUB_OUTPUT
          echo "all_tests=[]" >> $GITHUB_OUTPUT

  compare-results:
    needs: [test-source-branch, test-target-branch]
    runs-on: ${{ inputs.runs_on }}
    outputs:
      has_regressions: ${{ needs.perform-regression-analysis.outputs.has_regressions }}
      regression_count: ${{ needs.perform-regression-analysis.outputs.regression_count }}

    steps:
      - name: Install bc
        run: |
          sudo apt-get update -y
          sudo apt-get install -y bc

      - name: Check for collection errors
        run: |
          # Create analysis debug file
          exec 3>&1 4>&2
          exec 1> >(tee -a debug_comparison_analysis.log) 2>&1

          echo "Retrieving collection error status information"
          PR_COLLECTION_ERRORS="${{ needs.test-source-branch.outputs.collection_errors }}"
          PR_NO_TESTS="${{ needs.test-source-branch.outputs.no_tests_found }}"
          PR_ERROR_TYPE="${{ needs.test-source-branch.outputs.error_type }}"
          PR_ERROR_DETAILS="${{ needs.test-source-branch.outputs.error_details }}"
          TARGET_COLLECTION_ERRORS="${{ needs.test-target-branch.outputs.collection_errors }}"

          echo "PR branch collection errors: $PR_COLLECTION_ERRORS"
          echo "PR branch no tests found: $PR_NO_TESTS"
          echo "PR branch error type: $PR_ERROR_TYPE"
          echo "Target branch collection errors: $TARGET_COLLECTION_ERRORS"

          # Restore stdout/stderr for GitHub Actions
          exec 1>&3 2>&4

          # Distinct error handling for PR branch
          if [[ "$PR_COLLECTION_ERRORS" == "true" ]]; then
            echo "::error::Test discovery errors in PR branch: $PR_ERROR_TYPE"
            echo "::error::$PR_ERROR_DETAILS"
            echo "❌ PR branch has test discovery errors. Python modules could not be imported correctly."
            exit 1
          fi

          if [[ "$PR_NO_TESTS" == "true" ]]; then
            echo "::error::No tests were found in the PR branch"
            echo "❌ PR branch has no tests detected. Please add test files that match pytest's discovery pattern."
            exit 1
          fi

          # Warning for target branch issues (not a failure)
          if [[ "$TARGET_COLLECTION_ERRORS" == "true" ]]; then
            echo "⚠️ Target branch has test discovery errors. Tests will still be compared but results may not be accurate."
          fi

          if [[ "${{ needs.test-target-branch.outputs.no_tests_found }}" == "true" ]]; then
            echo "⚠️ Target branch has no tests detected. PR branch tests will still be evaluated."
          fi

      # Split the regression check into separate steps for better control
      # NOTE: These regression analysis steps are redundant with the perform-regression-analysis job
      # but are kept for now to ensure backward compatibility. They should be removed in a future cleanup.
      - name: Run regression analysis
        run: |
          # Create analysis debug file
          exec 3>&1 4>&2
          exec 1> >(tee -a debug_regression_analysis.log) 2>&1

          echo "Running regression analysis..."

          python3 - << 'EOF'
          import json
          import os

          try:
              # Parse the inputs
              target_passing_str = '''${{ needs.test-target-branch.outputs.passing_tests }}'''
              pr_failing_str = '''${{ needs.test-source-branch.outputs.failing_tests }}'''
              
              # Parse JSON 
              target_passing = json.loads(target_passing_str) if target_passing_str and target_passing_str != '[]' else []
              pr_failing = json.loads(pr_failing_str) if pr_failing_str and pr_failing_str != '[]' else []
              
              print(f"Parsed {len(target_passing)} passing tests from target branch")
              print(f"Parsed {len(pr_failing)} failing tests from PR branch")
              
              # Find regressions using set operations
              target_passing_set = set(target_passing)
              pr_failing_set = set(pr_failing)
              regression_tests = list(target_passing_set.intersection(pr_failing_set))
              
              # Write results to file if there are regressions
              if regression_tests:
                  print(f"Found {len(regression_tests)} regression(s)!")
                  
                  with open("regression_details.txt", "w") as f:
                      f.write(f"Found {len(regression_tests)} tests that were passing in target branch but now failing in PR branch:\\n\\n")
                      for idx, test in enumerate(sorted(regression_tests), 1):
                          f.write(f"{idx}. {test}\\n")
                  print("Regression details written to file")
              else:
                  print("No regressions found")
          except Exception as e:
              print(f"Error in regression analysis: {e}")
              import traceback
              print(traceback.format_exc())
          EOF

          # Restore stdout/stderr for GitHub Actions
          exec 1>&3 2>&4

          echo "Regression analysis completed"

      - name: Check for regression details file
        id: check-regressions
        run: |
          _has_regressions="false"
          _regression_count="0"

          if [ -f "regression_details.txt" ]; then
            echo "Regression details file exists"
            # Count regression lines (lines starting with a number and period)
            _current_count=$(grep -c "^[0-9]\+\." regression_details.txt || echo "0")
            echo "Found $_current_count regression items in file"

            if [ "$_current_count" -gt 0 ]; then
              _has_regressions="true"
              _regression_count="$_current_count"
              echo "::error::Test Regressions Found: $_regression_count test(s) that were passing in target branch are now **failing** in PR branch."
              echo "Regression details:"
              cat regression_details.txt
            else
              # File exists but no regressions counted (e.g. empty or malformed)
              _has_regressions="false"
              _regression_count="0"
            fi
          else
            echo "No regression details file found - no regressions detected"
            _has_regressions="false"
            _regression_count="0"
          fi

          echo "HAS_REGRESSIONS=$_has_regressions" >> $GITHUB_OUTPUT
          echo "REGRESSION_COUNT=$_regression_count" >> $GITHUB_OUTPUT

          if [[ "$_has_regressions" == "false" ]]; then
            if [ -f regression_details.txt ] && [ "$_has_regressions" == "false" ]; then
               echo "::notice::Regression details file (regression_details.txt) was found but no valid regression entries were counted by this step, or the file was empty."
            else
               echo "No test regressions detected by this step."
            fi
          fi

      - name: Upload regression details artifact
        if: steps.check-regressions.outputs.HAS_REGRESSIONS == 'true' && steps.check-regressions.outputs.REGRESSION_COUNT > 0
        uses: actions/upload-artifact@v4
        with:
          # The artifact name from reusable-regression-analyzer.yml
          name: regression_details_pr_${{ github.event.pull_request.number || github.run_id }}_tests
          path: regression_details.txt
          retention-days: 1

      - name: Check for skip/xfail regressions
        run: |
          # Create analysis debug file
          exec 3>&1 4>&2
          exec 1> >(tee -a debug_skip_xfail_analysis.log) 2>&1

          echo "Checking for tests that were passing in target but are now skipped or xfailed in PR"

          python3 - << 'EOF'
          import json
          import os

          try:
              # Parse the inputs
              target_passing_str = '''${{ needs.test-target-branch.outputs.passing_tests }}'''
              pr_skipped_str = '''${{ needs.test-source-branch.outputs.skipped_tests }}'''
              pr_xfailed_str = '''${{ needs.test-source-branch.outputs.xfailed_tests }}'''
              pr_skipped_with_reasons_str = '''${{ needs.test-source-branch.outputs.skipped_tests_with_reasons }}'''
              pr_xfailed_with_reasons_str = '''${{ needs.test-source-branch.outputs.xfailed_tests_with_reasons }}'''
              
              # Parse JSON 
              target_passing = json.loads(target_passing_str) if target_passing_str and target_passing_str != '[]' else []
              pr_skipped = json.loads(pr_skipped_str) if pr_skipped_str and pr_skipped_str != '[]' else []
              pr_xfailed = json.loads(pr_xfailed_str) if pr_xfailed_str and pr_xfailed_str != '[]' else []
              pr_skipped_with_reasons = json.loads(pr_skipped_with_reasons_str) if pr_skipped_with_reasons_str and pr_skipped_with_reasons_str != '{}' else {}
              pr_xfailed_with_reasons = json.loads(pr_xfailed_with_reasons_str) if pr_xfailed_with_reasons_str and pr_xfailed_with_reasons_str != '{}' else {}
              
              print(f"Parsed {len(target_passing)} passing tests from target branch")
              print(f"Parsed {len(pr_skipped)} skipped tests from PR branch")
              print(f"Parsed {len(pr_xfailed)} xfailed tests from PR branch")
              print(f"Parsed {len(pr_skipped_with_reasons)} skipped tests with reasons")
              print(f"Parsed {len(pr_xfailed_with_reasons)} xfailed tests with reasons")
              
              # Find tests that were passing in target but are now skipped or xfailed in PR
              target_passing_set = set(target_passing)
              pr_skipped_set = set(pr_skipped)
              pr_xfailed_set = set(pr_xfailed)
              
              passing_to_skipped = list(target_passing_set.intersection(pr_skipped_set))
              passing_to_xfailed = list(target_passing_set.intersection(pr_xfailed_set))
              
              total_skip_xfail_regressions = len(passing_to_skipped) + len(passing_to_xfailed)
              
              if total_skip_xfail_regressions > 0:
                  print(f"Found {total_skip_xfail_regressions} tests that were passing in target but are now skipped/xfailed in PR!")
                  
                  # Build comprehensive warning message
                  warning_parts = [f"Skip/XFail Analysis: {total_skip_xfail_regressions} test(s) that were passing in target branch are now being skipped or xfailed in PR branch."]
                  
                  if passing_to_skipped:
                      warning_parts.append(f"Tests now SKIPPED ({len(passing_to_skipped)}):")
                      for idx, test in enumerate(sorted(passing_to_skipped), 1):
                          reason = pr_skipped_with_reasons.get(test, 'No reason provided')
                          warning_parts.append(f"  {idx}. {test} - Reason: {reason}")
                  
                  if passing_to_xfailed:
                      warning_parts.append(f"Tests now XFAILED ({len(passing_to_xfailed)}):")
                      for idx, test in enumerate(sorted(passing_to_xfailed), 1):
                          reason = pr_xfailed_with_reasons.get(test, 'No reason provided')
                          warning_parts.append(f"  {idx}. {test} - Reason: {reason}")
                  
                  warning_parts.append("While these changes don't fail the workflow, they indicate tests that were working before are now being bypassed. Please review these tests to ensure this is intentional.")
                  
                  # Print as single warning annotation
                  combined_warning = " ".join(warning_parts)
                  print(f"::warning::{combined_warning}")
              else:
                  print("No skip/xfail regressions found - all previously passing tests are still running.")
          except Exception as e:
              print(f"Error in skip/xfail analysis: {e}")
              import traceback
              print(traceback.format_exc())
          EOF

          # Restore stdout/stderr for GitHub Actions
          exec 1>&3 2>&4

          echo "Skip/xfail regression analysis completed"

      - name: Check for test additions and removals
        run: |
          # Create analysis debug file
          exec 3>&1 4>&2
          exec 1> >(tee -a debug_test_changes_analysis.log) 2>&1

          echo "Checking for test additions and removals between target and PR branches"

          python3 - << 'EOF'
          import json
          import os

          try:
              # Parse the inputs
              target_all_str = '''${{ needs.test-target-branch.outputs.all_tests }}'''
              pr_all_str = '''${{ needs.test-source-branch.outputs.all_tests }}'''
              
              # Parse JSON 
              target_all = json.loads(target_all_str) if target_all_str and target_all_str != '[]' else []
              pr_all = json.loads(pr_all_str) if pr_all_str and pr_all_str != '[]' else []
              
              print(f"Parsed {len(target_all)} total tests from target branch")
              print(f"Parsed {len(pr_all)} total tests from PR branch")
              
              # Find test additions and removals using set operations
              target_all_set = set(target_all)
              pr_all_set = set(pr_all)
              
              removed_tests = list(target_all_set - pr_all_set)  # In target but not in PR
              added_tests = list(pr_all_set - target_all_set)    # In PR but not in target
              
              # Report removed tests (warnings)
              if removed_tests:
                  print(f"Found {len(removed_tests)} tests that were removed from target branch!")
                  
                  # Build comprehensive removal warning
                  removal_parts = [f"Test Removal Analysis: {len(removed_tests)} test(s) that existed in target branch are missing from PR branch."]
                  removal_parts.append(f"REMOVED Tests ({len(removed_tests)}):")
                  for idx, test in enumerate(sorted(removed_tests), 1):
                      removal_parts.append(f"  {idx}. {test}")
                  removal_parts.append("These test removals should be reviewed to ensure they are intentional. If tests were renamed or moved, this may show as removal + addition.")
                  
                  # Print as single warning annotation
                  combined_removal_warning = " ".join(removal_parts)
                  print(f"::warning::{combined_removal_warning}")
              else:
                  print("No test removals detected.")
              
              # Report added tests (notifications/info)
              if added_tests:
                  print(f"Found {len(added_tests)} new tests added in PR branch!")
                  
                  # Build comprehensive addition notice
                  addition_parts = [f"Test Addition Analysis: {len(added_tests)} new test(s) have been added in the PR branch."]
                  addition_parts.append(f"NEW Tests ({len(added_tests)}):")
                  for idx, test in enumerate(sorted(added_tests), 1):
                      addition_parts.append(f"  {idx}. {test}")
                  addition_parts.append("New tests detected - this indicates expanded test coverage!")
                  
                  # Print as single notice annotation
                  combined_addition_notice = " ".join(addition_parts)
                  print(f"::notice::{combined_addition_notice}")
              else:
                  print("No new tests detected in PR branch.")
              
              # Summary
              if not removed_tests and not added_tests:
                  print("Test suite composition is unchanged between target and PR branches.")
              else:
                  print(f"Test suite changes: {len(added_tests)} added, {len(removed_tests)} removed")
                  
          except Exception as e:
              print(f"Error in test addition/removal analysis: {e}")
              import traceback
              print(traceback.format_exc())
          EOF

          # Restore stdout/stderr for GitHub Actions
          exec 1>&3 2>&4

          echo "Test addition/removal analysis completed"

      - name: Compare test results
        run: |
          echo "Test Results Summary:"
          echo "Target branch (${{ inputs.target_branch_to_compare }}): ${{ needs.test-target-branch.outputs.passed }}/${{ needs.test-target-branch.outputs.total }} tests passed (${{ needs.test-target-branch.outputs.percentage }}%)"
          echo "PR branch: ${{ needs.test-source-branch.outputs.passed }}/${{ needs.test-source-branch.outputs.total }} tests passed (${{ needs.test-source-branch.outputs.percentage }}%)"

          if [[ "${{ needs.test-source-branch.outputs.total }}" == "0" ]]; then
            echo "::error::No tests were found in the PR branch"
            echo "❌ PR branch has no tests detected. Please add test files that match pytest's discovery pattern."
            exit 1
          fi

          PR_PASSED=${{ needs.test-source-branch.outputs.passed }}
          TARGET_PASSED=${{ needs.test-target-branch.outputs.passed }}
          PR_PERCENTAGE=${{ needs.test-source-branch.outputs.percentage }}
          TARGET_PERCENTAGE=${{ needs.test-target-branch.outputs.percentage }}
          PR_TOTAL=${{ needs.test-source-branch.outputs.total }}
          TARGET_TOTAL=${{ needs.test-target-branch.outputs.total }}

          # Handle case where target has no tests
          if [[ "$TARGET_TOTAL" == "0" ]]; then
            if [[ "$PR_PASSED" -gt 0 ]]; then
              echo "✅ PR branch has tests and some are passing (target branch has no tests)"
              exit 0
            else
              echo "❌ PR branch has no passing tests"
              echo "  - Pass percentage: $PR_PERCENTAGE%"
              exit 1
            fi
          fi

          # Fail if any tests passed in target branch but now fail in PR branch
          if [[ "${{ needs.perform-regression-analysis.outputs.has_regressions }}" == "true" ]]; then
            echo "❌ PR branch has test regressions from target branch"
            REGRESSION_COUNT_VAL=${{ needs.perform-regression-analysis.outputs.regression_count }}
            echo "  - $REGRESSION_COUNT_VAL tests that were passing in target branch are now failing"

            echo "### :x: Test Regressions Detected!" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**$REGRESSION_COUNT_VAL test(s) that were passing in the target branch are now failing in the PR branch.**" >> $GITHUB_STEP_SUMMARY
            echo "This job (\`compare-results\`) has been marked as failed due to these regressions." >> $GITHUB_STEP_SUMMARY

            if [ -f regression_details.txt ]; then # Check if file exists (it should from previous step)
                echo "Failing tests (regressions) list:"
                cat regression_details.txt
            else
                echo "::warning::Regression details file (regression_details.txt) not found in Compare test results step. It might be available as an artifact from the 'perform-regression-analysis' job."
            fi
            exit 1
          fi

          # Continue with the original comparison if no regressions
          if (( $(echo "$PR_PASSED >= $TARGET_PASSED" | bc -l) )) && (( $(echo "$PR_PERCENTAGE >= $TARGET_PERCENTAGE" | bc -l) )); then
            echo "✅ PR branch has equal or better test results than target branch"
            
            # Additional verbose information about improvement
            if (( $(echo "$PR_PASSED > $TARGET_PASSED" | bc -l) )); then
              IMPROVEMENT=$(( $PR_PASSED - $TARGET_PASSED ))
              echo "  - Improvement: $IMPROVEMENT more passing tests than target branch"
            fi
            
            if (( $(echo "$PR_PERCENTAGE > $TARGET_PERCENTAGE" | bc -l) )); then
              PERCENTAGE_IMPROVEMENT=$(echo "$PR_PERCENTAGE - $TARGET_PERCENTAGE" | bc -l)
              echo "  - Percentage improvement: +${PERCENTAGE_IMPROVEMENT}% compared to target branch"
            fi
            
            exit 0
          else
            echo "❌ PR branch has worse test results than target branch"
            echo "  - Passed tests: $PR_PASSED vs $TARGET_PASSED on target branch"
            echo "  - Pass percentage: $PR_PERCENTAGE% vs $TARGET_PERCENTAGE% on target branch"
            
            # Add to job summary for general comparison failure
            echo "### :x: Test Comparison Failed" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "The PR branch has worse test results than the target branch:" >> $GITHUB_STEP_SUMMARY
            echo "- Passed tests: $PR_PASSED (PR) vs $TARGET_PASSED (Target)" >> $GITHUB_STEP_SUMMARY
            echo "- Pass percentage: $PR_PERCENTAGE% (PR) vs $TARGET_PERCENTAGE% (Target)" >> $GITHUB_STEP_SUMMARY
            
            # Calculate regression metrics
            if (( $(echo "$PR_PASSED < $TARGET_PASSED" | bc -l) )); then
              REGRESSION=$(( $TARGET_PASSED - $PR_PASSED ))
              echo "  - Regression: $REGRESSION fewer passing tests than target branch"
            fi
            
            if (( $(echo "$PR_PERCENTAGE < $TARGET_PERCENTAGE" | bc -l) )); then
              PERCENTAGE_REGRESSION=$(echo "$TARGET_PERCENTAGE - $PR_PERCENTAGE" | bc -l)
              echo "  - Percentage regression: -${PERCENTAGE_REGRESSION}% compared to target branch"
            fi
            
            exit 1
          fi

      - name: Upload comparison analysis logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: comparison_analysis_logs_${{ github.event.pull_request.number || github.run_id }}
          path: debug_*_analysis.log
          retention-days: 3
          if-no-files-found: ignore

  perform-regression-analysis:
    needs: [test-source-branch, test-target-branch]
    uses: ./.github/workflows/meta-regression-analysis.yml
    with:
      item_type_singular: "test"
      item_type_plural: "tests"
      pr_number: ${{ github.event.pull_request.number }}
      run_id: ${{ github.run_id }}
      target_branch_artifact_name: target_branch_test_data_${{ github.event.pull_request.number || github.run_id }}
      pr_branch_artifact_name: pr_branch_test_data_${{ github.event.pull_request.number || github.run_id }}
    # Secrets are not needed for this reusable workflow currently
    # secrets: inherit

  # Conditionally run notification job only if needed
  prepare-notification:
    name: Prepare Notification Data
    needs:
      [
        lint,
        test-source-branch,
        test-target-branch,
        compare-results,
        perform-regression-analysis,
      ]
    # Notify on collection errors, no tests found, compare result failure, or if regressions are detected
    if: |
      always() &&
      (
        needs.test-source-branch.outputs.collection_errors == 'true' ||
        needs.test-source-branch.outputs.no_tests_found == 'true' ||
        needs.compare-results.result == 'failure' ||
        needs.perform-regression-analysis.outputs.has_regressions == 'true'
      )
    runs-on: ${{ inputs.runs_on }}
    outputs:
      message_body: ${{ steps.construct_notification.outputs.message_body_out }}
      ping_user_ids: ${{ steps.construct_notification.outputs.ping_user_ids_out }}
      artifact_path: ${{ steps.construct_notification.outputs.artifact_path_out }}
      should_notify: "true"
      webhook_available_for_alert: ${{ steps.check_webhook_availability.outputs.webhook_available }}

    steps:
      - name: Check for Discord Webhook URL
        id: check_webhook_availability
        run: |
          if [ -z "${{ secrets.DISCORD_WEBHOOK_URL }}" ]; then
            echo "::notice::DISCORD_WEBHOOK_URL secret is not set. Discord notifications will likely be skipped by the alert workflow if it relies on this secret."
            echo "webhook_available=false" >> $GITHUB_OUTPUT
          else
            echo "::debug::DISCORD_WEBHOOK_URL secret is present."
            echo "webhook_available=true" >> $GITHUB_OUTPUT
          fi
      - name: Download regression details (if any)
        id: download_regressions
        if: needs.perform-regression-analysis.outputs.has_regressions == 'true' && needs.perform-regression-analysis.outputs.regression_count > 0
        uses: actions/download-artifact@v4
        with:
          # The artifact name from reusable-regression-analyzer.yml
          name: regression_details_pr_${{ github.event.pull_request.number || github.run_id }}_tests
          path: . # Download to current directory
        continue-on-error: true

      - name: Construct Discord Notification
        id: construct_notification
        env:
          LINT_RESULT: ${{ needs.lint.result }}
          SOURCE_TEST_RESULT: ${{ needs.test-source-branch.result }}
          TARGET_TEST_RESULT: ${{ needs.test-target-branch.result }}
          COMPARE_RESULT: ${{ needs.compare-results.result }}
          PR_COLLECTION_ERRORS: ${{ needs.test-source-branch.outputs.collection_errors }}
          PR_NO_TESTS_FOUND: ${{ needs.test-source-branch.outputs.no_tests_found }}
          PR_ERROR_TYPE: ${{ needs.test-source-branch.outputs.error_type }}
          PR_ERROR_DETAILS_TRUNCATED: ${{ needs.test-source-branch.outputs.error_details }}
          HAS_REGRESSIONS: ${{ needs.perform-regression-analysis.outputs.has_regressions }}
          REGRESSION_COUNT: ${{ needs.perform-regression-analysis.outputs.regression_count }}
          PR_TOTAL_TESTS: ${{ needs.test-source-branch.outputs.total }}
          PR_PASSED_TESTS: ${{ needs.test-source-branch.outputs.passed }}
          PR_PERCENTAGE: ${{ needs.test-source-branch.outputs.percentage }}
          TARGET_TOTAL_TESTS: ${{ needs.test-target-branch.outputs.total }}
          TARGET_PASSED_TESTS: ${{ needs.test-target-branch.outputs.passed }}
          TARGET_PERCENTAGE: ${{ needs.test-target-branch.outputs.percentage }}
          PR_NUMBER: ${{ github.event.pull_request.number }}
          PR_TITLE: ${{ github.event.pull_request.title }}
          PR_URL: ${{ github.event.pull_request.html_url }}
          TARGET_BRANCH_NAME: ${{ inputs.target_branch_to_compare }}
          PR_BRANCH_NAME: ${{ github.head_ref }}
          REPO_URL: ${{ github.server_url }}/${{ github.repository }}
          ACTION_RUN_URL: ${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}
          GH_ASSIGNEES_JSON: ${{ toJson(github.event.pull_request.assignees) }}
          USER_MAP_JSON: ${{ secrets.DISCORD_USER_MAP || '{}' }}
          REGRESSION_FILE_PATH: "regression_details.txt"
          DOWNLOAD_REGRESSIONS_OUTCOME: ${{ steps.download_regressions.outcome }}
          INPUT_PING_LATEST_COMMITTER: ${{ inputs.ping_latest_committer }}
        run: |
          # Create debug file for detailed notification construction
          exec 3>&1 4>&2
          exec 1> >(tee -a debug_notification_construction.log) 2>&1

          MESSAGE_LINES=() # Use an array to build message lines
          PING_KEYS_OUTPUT="" # Will be comma-separated GitHub logins
          ARTIFACT_PATH_OUTPUT=""

          echo "Raw GH_ASSIGNEES_JSON value: [$GH_ASSIGNEES_JSON]"
          echo "Raw USER_MAP_JSON value: [$USER_MAP_JSON]"

          # 1. Determine Pings - Collect GitHub Logins to pass to alert-discord.yml
          # Initialize PING_KEYS_OUTPUT
          PING_KEYS_OUTPUT=""

          # Add assignees to PING_KEYS_OUTPUT
          if [ -n "$USER_MAP_JSON" ] && [ "$USER_MAP_JSON" != "{}" ] && command -v jq &> /dev/null; then
            ASSIGNEE_LOGINS_ARRAY=($(echo "$GH_ASSIGNEES_JSON" | jq -r '.[].login // empty'))
            echo "GH_ASSIGNEES_JSON received: $GH_ASSIGNEES_JSON"
            echo "Extracted ASSIGNEE_LOGINS_ARRAY: (${ASSIGNEE_LOGINS_ARRAY[*]})"
            echo "Count of assignees extracted: ${#ASSIGNEE_LOGINS_ARRAY[@]}"

            MAPPED_ASSIGNEE_COUNT=0
            TEMP_PING_KEYS=()

            for assignee_login in "${ASSIGNEE_LOGINS_ARRAY[@]}"; do
              if [ -z "$assignee_login" ]; then
                echo "Skipping empty assignee login."
                continue
              fi
              echo "Processing assignee for ping: '$assignee_login'"
              # Check if this assignee_login exists as a key in USER_MAP_JSON
              if echo "$USER_MAP_JSON" | jq -e --arg K "$assignee_login" '.[$K]' > /dev/null; then
                echo "Assignee '$assignee_login' FOUND in USER_MAP_JSON."
                TEMP_PING_KEYS+=("$assignee_login")
                MAPPED_ASSIGNEE_COUNT=$((MAPPED_ASSIGNEE_COUNT + 1))
              else
                echo "Assignee '$assignee_login' NOT FOUND in USER_MAP_JSON."
              fi
            done

            echo "Total assignees found in USER_MAP_JSON and added to pings: $MAPPED_ASSIGNEE_COUNT"

            if [ ${#TEMP_PING_KEYS[@]} -gt 0 ]; then
              PING_KEYS_OUTPUT=$(IFS=,; echo "${TEMP_PING_KEYS[*]}")
              echo "Initial PING_KEYS_OUTPUT from assignees: [$PING_KEYS_OUTPUT]"
            else
              echo "No assignees found or GH_ASSIGNEES_JSON was empty, or no assignees were found in USER_MAP_JSON."
            fi
          elif [ -n "$USER_MAP_JSON" ] && [ "$USER_MAP_JSON" != "{}" ] && ! command -v jq &> /dev/null; then
            echo "::warning::jq is not available. Cannot determine GitHub users (assignees) for pings."
          else
            echo "No user map JSON or jq not found. PING_KEYS_OUTPUT (from assignees) will be empty."
          fi

          # Add latest committer if INPUT_PING_LATEST_COMMITTER is true
          if [[ "$INPUT_PING_LATEST_COMMITTER" == "true" ]]; then
            echo "INPUT_PING_LATEST_COMMITTER is true. Attempting to fetch latest committer for PR #${PR_NUMBER}."
            if command -v gh &> /dev/null && [ -n "$PR_NUMBER" ]; then
              LATEST_COMMITTER_LOGIN_RAW=$(gh pr view "$PR_NUMBER" --json commits --jq '.commits[-1].author.login' 2>/dev/null || echo "")
              
              if [ -n "$LATEST_COMMITTER_LOGIN_RAW" ] && [ "$LATEST_COMMITTER_LOGIN_RAW" != "null" ]; then
                # Apply bot filter (e.g., names ending in [bot] or -bot)
                LATEST_COMMITTER_LOGIN=$(echo "$LATEST_COMMITTER_LOGIN_RAW" | grep -v -E -i '(\[bot\]$|-bot$)' || echo "")
                
                if [ -n "$LATEST_COMMITTER_LOGIN" ]; then
                  echo "Latest committer identified: $LATEST_COMMITTER_LOGIN"
                  
                  # Check if this committer is already in PING_KEYS_OUTPUT
                  ALREADY_IN_LIST=0
                  if [ -n "$PING_KEYS_OUTPUT" ]; then # Only check if PING_KEYS_OUTPUT is not empty
                    IFS=',' read -ra PING_ARRAY <<< "$PING_KEYS_OUTPUT"
                    for key in "${PING_ARRAY[@]}"; do
                      if [[ "$key" == "$LATEST_COMMITTER_LOGIN" ]]; then
                        ALREADY_IN_LIST=1
                        break
                      fi
                    done
                  fi
                  
                  if [[ "$ALREADY_IN_LIST" -eq 0 ]]; then
                    if [ -z "$PING_KEYS_OUTPUT" ]; then
                      PING_KEYS_OUTPUT="$LATEST_COMMITTER_LOGIN"
                    else
                      PING_KEYS_OUTPUT="$PING_KEYS_OUTPUT,$LATEST_COMMITTER_LOGIN"
                    fi
                    echo "Added latest committer '$LATEST_COMMITTER_LOGIN' to PING_KEYS_OUTPUT. New list: [$PING_KEYS_OUTPUT]"
                  else
                    echo "Latest committer '$LATEST_COMMITTER_LOGIN' is already in PING_KEYS_OUTPUT (likely an assignee)."
                  fi
                else
                  echo "Latest committer login '$LATEST_COMMITTER_LOGIN_RAW' was filtered out (likely a bot or pattern match) or empty after filter."
                fi
              else
                echo "No latest committer login found for PR #$PR_NUMBER from gh command, or login was null."
              fi
            else
              if ! command -v gh &> /dev/null; then
                echo "::warning::gh command not available. Cannot fetch latest committer."
              fi
              if [ -z "$PR_NUMBER" ]; then
                echo "::warning::PR_NUMBER is not set (event might not be a pull_request). Cannot fetch latest committer."
              fi
            fi
          fi

          # Restore stdout/stderr for GitHub Actions to show final summary
          exec 1>&3 2>&4

          # Make this a standard echo for better visibility of the final list
          echo "Final Ping Keys Output (GitHub Logins from test-pytest.yml): [$PING_KEYS_OUTPUT]"
          echo "ping_user_ids_out=$PING_KEYS_OUTPUT" >> $GITHUB_OUTPUT

          # Store branch names in variables with proper quoting
          PR_BRANCH="${PR_BRANCH_NAME:-unknown}"
          TARGET_BRANCH="${TARGET_BRANCH_NAME:-unknown}"

          # 2. Construct Message Body
          MESSAGE_LINES+=("**Pytest Comparison & Regression Analysis for PR [#${PR_NUMBER}: ${PR_TITLE}](${PR_URL})**")
          MESSAGE_LINES+=("Branch: [\`${PR_BRANCH}\`](${REPO_URL}/tree/${PR_BRANCH}) against [\`${TARGET_BRANCH}\`](${REPO_URL}/tree/${TARGET_BRANCH})")
          MESSAGE_LINES+=("---")

          # Job Status Summary
          MESSAGE_LINES+=("**Job Status:**")
          LINT_STATUS="Success"
          if [[ "$LINT_RESULT" == "failure" ]]; then LINT_STATUS="Failed"; elif [[ "$LINT_RESULT" == "skipped" ]]; then LINT_STATUS="Skipped"; fi
          MESSAGE_LINES+=("- Linting: $LINT_STATUS")

          SOURCE_TEST_STATUS="Success"
          if [[ "$SOURCE_TEST_RESULT" == "failure" ]]; then SOURCE_TEST_STATUS="Failed"; elif [[ "$SOURCE_TEST_RESULT" == "skipped" ]]; then SOURCE_TEST_STATUS="Skipped"; fi
          MESSAGE_LINES+=("- PR Branch Tests (\`${PR_BRANCH}\`): $SOURCE_TEST_STATUS")

          TARGET_TEST_STATUS="Success"
          if [[ "$TARGET_TEST_RESULT" == "failure" ]]; then TARGET_TEST_STATUS="Failed"; elif [[ "$TARGET_TEST_RESULT" == "skipped" ]]; then TARGET_TEST_STATUS="Skipped"; fi
          MESSAGE_LINES+=("- Target Branch Tests (\`${TARGET_BRANCH}\`): $TARGET_TEST_STATUS")

          COMPARE_STATUS="Success"
          if [[ "$COMPARE_RESULT" == "failure" ]]; then COMPARE_STATUS="Failed"; elif [[ "$COMPARE_RESULT" == "skipped" ]]; then COMPARE_STATUS="Skipped"; fi
          MESSAGE_LINES+=("- Comparison & Regression: $COMPARE_STATUS")
          MESSAGE_LINES+=("---")

          # Test Discovery Issues in PR Branch
          if [[ "$PR_COLLECTION_ERRORS" == "true" ]]; then
            MESSAGE_LINES+=("**:red_circle: ERROR: Test Discovery Failed in PR Branch (\`${PR_BRANCH}\`)**")
            MESSAGE_LINES+=("  - Type: \`${PR_ERROR_TYPE}\`")
            MESSAGE_LINES+=("  - Details: \`\`\`${PR_ERROR_DETAILS_TRUNCATED}\`\`\`")
            MESSAGE_LINES+=("  - This usually indicates import errors or syntax issues preventing tests from being collected.")
          elif [[ "$PR_NO_TESTS_FOUND" == "true" ]]; then
            MESSAGE_LINES+=("**:warning: WARNING: No Tests Found in PR Branch (\`${PR_BRANCH}\`)**")
            MESSAGE_LINES+=("  - Pytest did not discover any test files matching its patterns.")
            MESSAGE_LINES+=("  - Ensure your test files are correctly named (e.g., \`test_*.py\` or \`*_test.py\`) and located.")
          fi

          # Regression Analysis Summary
          if [[ "$HAS_REGRESSIONS" == "true" ]]; then
            MESSAGE_LINES+=("**:red_circle: REGRESSIONS DETECTED**")
            MESSAGE_LINES+=("  - **${REGRESSION_COUNT} test(s)** that were passing in \`${TARGET_BRANCH}\` are now **failing** in \`${PR_BRANCH}\`.")
            
            # Calculate current message length
            CURRENT_MESSAGE=$(printf "%s\\n" "${MESSAGE_LINES[@]}")
            CURRENT_LENGTH=${#CURRENT_MESSAGE}
            
            if [ -f "$REGRESSION_FILE_PATH" ] && [[ "$DOWNLOAD_REGRESSIONS_OUTCOME" == "success" ]]; then
              # Read regression details
              REGRESSION_LIST=$(awk '/^[0-9]+\./ {sub(/^[0-9]+\. /, "- "); print}' "$REGRESSION_FILE_PATH")
              
              # Calculate length with regression details
              TEMP_MESSAGE="$CURRENT_MESSAGE"
              TEMP_MESSAGE+="\`\`\`"
              TEMP_MESSAGE+="$REGRESSION_LIST"
              TEMP_MESSAGE+="\`\`\`"
              TEMP_LENGTH=${#TEMP_MESSAGE}
              
              if [ $TEMP_LENGTH -le 2000 ]; then
                # If total length would be under 2000 chars, include in message
                MESSAGE_LINES+=("  - **Failed Tests (Regressions):**")
                MESSAGE_LINES+=("\`\`\`")
                MESSAGE_LINES+=("$REGRESSION_LIST")
                MESSAGE_LINES+=("\`\`\`")
                ARTIFACT_PATH_OUTPUT="" # No artifact if details are inline
              else
                # If would exceed 2000 chars, attach file instead
                MESSAGE_LINES+=("  - Details for the ${REGRESSION_COUNT} regressions are in the attached \`regression_details.txt\` file.")
                ARTIFACT_PATH_OUTPUT="$REGRESSION_FILE_PATH"
              fi
            else
              MESSAGE_LINES+=("  (Regression details file not found or download failed; cannot list specific regressions here.)")
              ARTIFACT_PATH_OUTPUT=""
            fi
          elif [[ "$COMPARE_RESULT" == "failure" ]] && [[ "$HAS_REGRESSIONS" != "true" ]]; then
            # This case handles general comparison failures NOT due to specific regressions
            MESSAGE_LINES+=("**:warning: TEST RESULTS DECLINED**")
            MESSAGE_LINES+=("  - The PR branch shows a decrease in test success compared to the target branch, but no specific regressions were identified by the \`meta-regression-analysis\` job.")
            MESSAGE_LINES+=("  - PR Branch (\`${PR_BRANCH}\`): **${PR_PASSED_TESTS}/${PR_TOTAL_TESTS} passed (${PR_PERCENTAGE}%)**")
            MESSAGE_LINES+=("  - Target Branch (\`${TARGET_BRANCH}\`): **${TARGET_PASSED_TESTS}/${TARGET_TOTAL_TESTS} passed (${TARGET_PERCENTAGE}%)**")
          elif [[ "$COMPARE_RESULT" == "success" ]] && [[ "$HAS_REGRESSIONS" != "true" ]]; then
             MESSAGE_LINES+=("**:white_check_mark: NO REGRESSIONS DETECTED**")
             MESSAGE_LINES+=("  - PR Branch (\`${PR_BRANCH}\`): **${PR_PASSED_TESTS}/${PR_TOTAL_TESTS} passed (${PR_PERCENTAGE}%)**")
             MESSAGE_LINES+=("  - Target Branch (\`${TARGET_BRANCH}\`): **${TARGET_PASSED_TESTS}/${TARGET_TOTAL_TESTS} passed (${TARGET_PERCENTAGE}%)**")
          fi

          MESSAGE_LINES+=("---")
          MESSAGE_LINES+=("[View Workflow Run](${ACTION_RUN_URL})")

          # Construct with actual newlines
          FINAL_MESSAGE_BODY=$(printf "%s\\n" "${MESSAGE_LINES[@]}")
          if [ ${#MESSAGE_LINES[@]} -gt 0 ]; then
            # Remove the very last actual newline
            FINAL_MESSAGE_BODY="${FINAL_MESSAGE_BODY%\\n}"
          fi

          echo "Final message body prepared in test-pytest.yml"

          echo "message_body_out<<EOF" >> $GITHUB_OUTPUT
          echo "$FINAL_MESSAGE_BODY" >> $GITHUB_OUTPUT
          echo "EOF" >> $GITHUB_OUTPUT

          echo "artifact_path_out=$ARTIFACT_PATH_OUTPUT" >> $GITHUB_OUTPUT

      - name: Upload notification construction debug logs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: notification_debug_logs_${{ github.event.pull_request.number || github.run_id }}
          path: debug_notification_construction.log
          retention-days: 3
          if-no-files-found: ignore

  # Even if webhook checks are handled inside the alert workflow,
  # we still need to pass the secret to satisfy GitHub's workflow validation
  notify-discord:
    name: Send Discord Notification
    needs: [prepare-notification]
    if: |
      always() &&
      needs.prepare-notification.outputs.should_notify == 'true' &&
      needs.prepare-notification.outputs.webhook_available_for_alert == 'true'
    uses: ./.github/workflows/alert-discord.yml
    with:
      message_body: ${{ needs.prepare-notification.outputs.message_body }}
      ping_user_ids: ${{ needs.prepare-notification.outputs.ping_user_ids }}
      artifact_paths: ${{ needs.prepare-notification.outputs.artifact_path }}
      should_notify: ${{ needs.prepare-notification.outputs.should_notify }}
      runs_on: ${{ inputs.runs_on }}
    secrets:
      DISCORD_WEBHOOK_URL: ${{ secrets.DISCORD_WEBHOOK_URL }}
      DISCORD_USER_MAP: ${{ secrets.DISCORD_USER_MAP }}
